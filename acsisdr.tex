%% This is file `elsarticle-template-2-harv.tex',
%%
%% Copyright 2009 Elsevier Ltd
%%
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%%
%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references
%%
%% $Id: elsarticle-template-2-harv.tex 155 2009-10-08 05:35:05Z rishi $
%% $URL: http://lenova.river-valley.com/svn/elsbst/trunk/elsarticle-template-2-harv.tex $
%%

%%\documentclass[preprint,authoryear,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:

%% Astronomy & Computing uses 5p
%% \documentclass[final,authoryear,5p,times]{elsarticle}
\documentclass[final,authoryear,5p,times,twocolumn]{elsarticle}

%% if you use PostScript figures in your article
%% use the graphics package for simple commands
%% \usepackage{graphics}
%% or use the graphicx package for more complicated commands
\usepackage{graphicx}
%% or use the epsfig package if you prefer to use the old commands
%% \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

\usepackage[pdftex,pdfpagemode={UseOutlines},bookmarks,bookmarksopen,colorlinks,linkcolor={blue},citecolor={green},urlcolor={red}]{hyperref}
\usepackage{hypernat}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
%% \usepackage{lineno}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon (default)
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   authoryear - selects author-year citations (default)
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%   longnamesfirst  -  makes first citation full author list
%%
%% \biboptions{longnamesfirst,comma}

% \biboptions{}

\journal{Astronomy \& Computing}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\title{Automated reduction of submillimetre single-dish heterodyne
  data from the James Clerk Maxwell Telescope using ORAC-DR}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{<author name>}
%% \address[label1]{<address>}
%% \address[label2]{<address>}

\author[jac,cornell]{Tim Jenness\corref{cor1}}
\ead{tjenness@cornell.edu}
\author[jac]{Malcolm J.\ Currie}
\ead{m.currie@jach.hawaii.edu}
\author[jac]{Remo P.\ J.\ Tilanus\fnref{rpt}}
\author[jac]{Brad Cavanagh}
\author[jac]{David S. Berry}
\author[jac]{Jamie Leech\fnref{jxl}}
\author[jac]{Luca Rizzi\fnref{lr}}

\cortext[cor1]{Corresponding author}
\fntext[rpt]{Present address: Leiden Observatory, PO Box 9513, 2300 RA
  Leiden, The Netherlands}
\fntext[jxl]{Present address: Department of Physics, University of
  Oxford, Denys Wilkinson Building, Keble Road, Oxford, OX1 3RH, UK}
\fntext[lr]{Present address: W.\ M.\ Keck Observatory, 65-1120 Mamalahoa Hwy, Kamuela,
  HI 96743, USA}

\address[jac]{Joint Astronomy Centre, 660 N.\ A`oh\=ok\=u Place, Hilo, HI
  96720, USA}
\address[cornell]{Department of Astronomy, Cornell University, Ithaca,
  NY 14853, USA}

\begin{abstract}
%% Text of abstract

With the advent of modern multi-receptor heterodyne instruments that
can result in observations generating thousands of spectra per minute it is
no longer feasible to reduce these data as individual spectra. This
paper describes how baselined, mosaicked data cubes can be created by
an automated pipeline in a reliable manner, including detection of bad
spectra, removal of transient effects and creation of high fidelity
moment maps.

\end{abstract}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

submillimeter astronomy \sep
methods: data analysis \sep
pipelines \sep
techniques: spectroscopic \sep
techniques: image processing \sep
James Clerk Maxwell Telescope

\end{keyword}

\end{frontmatter}

% \linenumbers

%% Journal abbreviations
\newcommand{\mnras}{Mon Not R Astron Soc}
\newcommand{\aap}{Astron Astrophys}
\newcommand{\pasp}{Pub Astron Soc Pacific}
\newcommand{\apj}{Astrophys J}
\newcommand{\qjras}{Quart J R Astron Soc}
\newcommand{\an}{Astron.\ Nach.}
\newcommand{\ijimw}{Int.\ J.\ Infrared \& Millimeter Waves}
\newcommand{\procspie}{Proc.\ SPIE}
\newcommand{\aspconf}{ASP Conf. Ser.}

%% Applications
\newcommand{\makecube}{\textsc{Makecube}}

%% main text
\section{Introduction}
\label{sec:intro}

As heterodyne receivers have progressed from single-receptor
instruments
\citep{1992IJIMW..13.1487P,1992IJIMW..13..647D,1992IJIMW..13.1827C} to
small focal plane arrays
\citep{2003SPIE.4855..322G,2004A&A...423.1171S} to 16 element arrays
such as HARP at JCMT \citep{2009MNRAS.399.1026B}, and beyond
\citep{2007stt..conf..264G}, and correlators have improved such that
we can easily obtain spectra at 10\,Hz with 8192 channels, data rates
have increased substantially such that it is now common-place to take
a short observation resulting in thousands of spectra. This is only
going to become more challenging with the advent of instruments with
64,000 channels and large format arrays such as CHAI on CCAT
\citep{2009ASPC..417..113R,GoldsmithCHAI2012}.

In submillimeter astronomy data reductions packages such as
\htmladdnormallinkfoot{CLASS}{http://www.iram.fr/IRAMFR/GILDAS} and
SPECX \citep{SPECX} were developed that worked well with
single-receptor instruments. Scripting interfaces and tools for
curating collections of spectra were insufficient as the data rates
increased and data pipelines \citep[e.g.,][]{1995ASPC...75..117W} and
algorithms that work on the full data set
\citep[e.g.,][]{2002ASPC..278..329M} were suggested. The ACSIS data
reduction system \citep{2000ASPC..216..502L,2000SPIE.4015..114H},
delivered to the JCMT in 2005, aimed to deal with the data rate issues
by providing a pipeline that sent the calibrated spectra, with
optional baselining, to a component that would place the spectra in to
a data cube. Although it was possible to store the raw data in CASA
measurement sets \citep{2012ASPC..461..849P}\footnote{At the time this
  was being developed CASA was known as AIPS++
  \citep{2004ASPC..314..468M}}, the data cube was the product that was
archived and taken away by the astronomer for further analysis. This
strategy was forced on us given the computer resources available when
ACSIS was being designed and developed and was known to have risks
associated with it. Coadding spectra into the cube allowed for
impressive ``data compression'' for stare and jiggle observing modes
although the gains were much less in scanning observing modes. There
were many downsides though. The observing system required that the
cube parameters be specified and initially it was felt that the
observer should select the parameters in the Observing Tool
\citep{2002ASPC..281..453F}. It was also necessary that the observer
specify the baseline regions and any frequency binning required. The
gridding and the frequency binning were irreversible and the observer
needed to ensure they did not make a mistake. Additionally, baselining
with anything other than a DC offset was also problematic as the fit
parameters for every spectrum were not stored and so could not be
reversed. The burden placed on the observer having to specify
everything, whilst simultaneously not making a mistake, was not
acceptable and by 2006 we realised that computers were fast enough and
storage large enough to be able to write the calibrated spectra
directly to disk and defer further data reduction to a pipeline.

\section{Heterodyne Data Reduction Pipeline}

It was decided that data reduction recipes would be written for the
existing ORAC-DR pipeline infrastructure in use at JCMT
\citep{2011tfa..confE..42J,2008AN....329..295C} with key requirements that
the output data cube should be specified from the telescope pointing
information and the location of each receptor on the sky, and that
baseline regions should be determined automatically by examining the
spectra as an ensemble. Furthermore, as progress was made on the
pipeline we additionally realised that we should also be able to
detect bad spectra and remove them from the coadd as well as doing
quality assurance tests. The latter were critical for the JCMT Legacy
Survey projects
\citep{2007PASP..119..855W,2009ApJ...693.1736W,2007PASP..119..102P}
who wanted to ensure they received data of consistent quality.

The JCMT heterodyne pipeline has two operating modes. The default
behaviour is for the pipeline to generate the best possible data
products without regard to efficiency. This is generally what is
required by scientists at their institutions and the mode we run in at
the JCMT Science Archive
\citep{2008ASPC..394..135G,2008ASPC..394..565J}. The other mode is a
cut-down version of the recipes that runs at the JCMT itself during
observing. This pipeline has constrained timing requirements and can
not perform many of the advanced processing features provided by the
main pipeline. It's role is to provide simple quality assurance
information and basic coadds to the observer and it will not be
discussed further in this paper.

\subsection{Determining Cube Parameters}
\label{sec:makecube}

Once we decided that we would not require that a PI specifies the
output pixel grid of the data cube in advance, it became clear that we
would have to provide software that would automatically determine the
pixel grid from the data itself. In the SMURF package data cubes are
created from calibrated spectra using the \makecube\
command. \makecube\ can be given an externally specified grid but also
has an \texttt{autogrid} option that leaves optimal grid determination
to the application itself.

Autogrid first projects the supplied sky positions into pixel
positions using an arbitrary tangent plane projection that has
1~arcmin square pixels with North upwards and the target position
(or the first supplied sky position if no target position is
available) at pixel (1,1).

It then projects each of these pixel positions onto a straight line
passing through pixel (1,1) at an angle, $\theta$, to North (see
Fig.~\ref{fig:autogrid}). This line is divided up into sections of
length 1~arcmin, and a histogram formed of the number of projected
positions that fall in each section. The amplitude and wavelength of
any periodicity in this histogram is found by looking at the
auto-correlation of the histogram (the amplitude is the
auto-correlation at zero shift, and the wavelength is the shift at the
first significant peak in the auto-correlation function).

\begin{figure}
\includegraphics[width=0.48\textwidth]{autogrid}
\caption{The Autogrid algorithm works by projecting each spectrum
  position onto a straight line at an arbitrary angle $\theta$, and then
  forming a histogram of the number of samples at each point along
  this line. At the optimal value of $\theta$, the projected positions
  line up, giving strong periodicity in the histogram.}
\label{fig:autogrid}
\end{figure}

This is repeated for many different line orientations in order to find
the value of $\theta$ (line orientation) that gives the strongest
periodicity in the histogram. This orientation is used as the
direction for the X pixel axis in the final grid. The corresponding
wavelength is used as the pixel spacing on the X axis. The wavelength
of the periodicity perpendicular to this direction is then found and
used as the pixel spacing on the Y axis.

Finally, we shift the reference pixel coordinates by up to one pixel
on each axis in order to minimise the sum of the squared distances
from each pixel projected sample position to the nearest pixel centre.

If the positions do not form a regular grid, an option is available to
create a 1-dimensional list of spectra in which the position of each
spectrum is recorded explicitly in a table using the FITS-WCS
\texttt{-TAB} algorithm \citep{2006A&A...446..747G}.

The output spectrum at each position on the grid is formed by
averaging the nearby input spectra. Various averaging schemes are
available, the simplest being to place each input spectrum entirely
into the nearest output pixel. Other schemes allow a 2-dimensional
kernel to be used to spread each input spectrum out over a range of
output pixels. Available kernels are those supported by the AST
library \citep{SUN211,2012ASPC..461..825B} and include a simple
bi-linear division between the four nearest neighbours, a Gaussian
kernel, and various flavours of kernels based on a sinc function.

When using nearest neighbour resampling there are multiple ways to
determine how bad pixels are propagated from input spectra into the
output cube. The \texttt{AND} scheme uses all input data (thus
reducing the noise in the output) and also minimises the number of bad
pixels in the output. However, the memory requirements of the
\texttt{AND} scheme can be excessive. For this reason, two other
schemes, \texttt{FIRST} and \texttt{OR}, are provided which greatly
reduce the memory requirements, at the expense either of introducing
more bad pixels into the output (\texttt{OR}) or producing higher
output noise levels (\texttt{FIRST}).

\begin{description}
\item[\texttt{FIRST}] The bad-pixel mask in each output spectrum is
  inherited from the first input spectrum that contributes to the
  output spectrum. Any subsequent input spectra that contribute to the
  same output spectrum but which have a different bad-pixel mask are
  ignored. So an output pixel will be bad if and only if the
  corresponding pixel in the first input NDF that contributes to it is
  bad. Since this scheme ignores entire input spectra if they do not
  conform to the expected bad-pixel mask, the noise in the output can
  be higher than using the other schemes. However, this scheme has the
  benefit of using much less memory than the \texttt{AND} scheme, and
  will in general produce fewer bad pixels in the output than the
  \texttt{OR} scheme.

\item[\texttt{OR}] The bad pixel mask in each output spectrum is the
  union (logical OR) of the bad pixel masks for all input spectra that
  contribute to the output spectrum. So an output pixel will be bad if
  any of the input pixels that contribute to it are bad.  This scheme
  will in general produce more bad output pixels than the
  \texttt{FIRST} scheme, but the non-bad output pixels will have a
  lower noise because, unlike \texttt{FIRST}, all the contributing
  input data are coadded to produce the good output pixels. Like
  \texttt{FIRST}, this scheme uses much less memory than \texttt{AND}.

\item[\texttt{ AND}] The bad pixel mask for each output spectrum is
  the intersection (logical AND) of the bad pixel masks for all input
  spectra that contribute to the output spectrum. So an output pixel
  will be bad only if all the input pixels that contribute to it are
  bad. This scheme will produce fewer bad output pixels and will also
  give lower output noise levels than \texttt{FIRST} or \texttt{OR},
  but at the expense of much greater memory requirements.

\end{description}

The pipeline processing defaults to using the \texttt{AND} scheme but
can be overridden if speed or memory are an issue.

One complication is that the data cube for a large area survey is
potentially extremely large and many software package do not support
data arrays with more than $2^{32}$ pixels. We overcome this by
supporting the ability for the output cube from \textsc{makecube} to
be split into tiles. These tiles share projection parameters and can
be recombined without further resampling if required. For pixel
spreading techniques that are susceptible to edge effects care is
taken to ensure that sufficient border is included in the tiles such
that the values in the output would be identical to those resulting
from a single output data cube. The border region is flagged in an
associated bit mask to ensure that it can be disabled during further
mosaicking or combination steps as the data in the border region will
have edge effects and is duplicating data found in other tiles.

\subsection{Sub-band merging}

In hybrid modes the correlator is configured to use multiple
hardware subsystems to process parts of the required bandwidth so
that they can be combined in the data reduction. The correlator is
usually configured such that the individual spectra overlap and also
have channels that are aligned to within a few per cent of a pixel.

The data acqusition system does not combine the subbands and so this
must be done by the pipeline. If the pipeline determines that it is
dealing with a hybrid mode observation the merging is done in
bulk. First the spectra are sorted by time (the ACSIS acquisition
computer does not guarantee that spectra will be written to files in
time order), then the overlap region is determined and the noisy ends
are trimmed before they are combined. The spectra can optionally have
their DC level adjusted before combining.

\textit{Could put up a plot of a DAS observation with all the
  overlapping subbands displayed. There is a reasonable plot on the
  TRAC system.}

\subsection{Automated Baseline Removal}

The real meat: smoothing, basic baseline subtraction, group coadd,
clump finding, unmakecube, source masking, iteration.

\begin{enumerate}
\item Create a cube coadd of all input observations
\item Smooth using 3-D tophat function with BLOCK
\item Use mfittrend in AUTO mode with first order baseline each
  spectrum in turn (single mode)
\item Mask out non-baseline region in original cube using the mask
  from fit to smoothed data.
\item Run mfittrend again AUTO=false single mode baseline=1 and
  subtract baseline from cube
\item \textit{Line forest baseline offset estimate seems to be for
    hybrid mode}
\item \textit{Create moment maps here but not relevant}
\item I'm actually a bit confused at this point. I thought that we
  determined a mask in 3D using CUPID and then we used it to make the
  time series masks. As written I can't see that happening. The mask
  is created for moment maps but then is not used elsewhere. The mask
  fed back to the time series seems to be the one initially guessed by
  mfittrend in REMOVE BASELINE THROUGH SMOOTHING.

\item A new time-series is then created for each input observation
  using the \textsc{unmakecube} command and the original time-series
  as templates. The result is a time-series with the baseline
  indicated in each spectrum.

\item The inverse of the baseline mask is then applied to the original
  observations to blank out the emission leaving a time-series which
  contains only baseline regions.

\item \textsc{mfittrend} is then used to calcualate a baseline using
  just the baseline regions. A spline or polynomial fit can be used
  here although the most common options are 3rd order polynomials or
  simple linear baselines.

\item The baseline fits are then subtracted from the original input
  observations.

\item A new cube is created by combining all the baselined observations.

\end{enumerate}

We can then repeat this step one more time by creating a new
mask from the newly-created coadd and then iterating. In practice,
given the linear properties of baseline addition, one iteration can be
enough. More iterations are needed if bad baseline detection and QA
parameters are recalculated which result in different numbers of
spectra being available each iteration.

\begin{figure}
\includegraphics[width=90mm]{flowchart}
\caption{Flow chart of the pipeline recipe.}
\label{fig:flowchart}
\end{figure}

\subsection{Determination of moment maps}

Moment maps can be compromised if excessive baseline is included in
the calculation. For example, in an integrated intensity calculation
the inclusion of all the baseline noise can hide a weak line. The data
reduction system has already calculated where the baseline regions are
for each spectrum and so for moment maps this mask is negated. This
leads to much improved fidelity and improves upon using a simple threshold or
the smoothing scheme used in the \textsc{momnt} command in AIPS
\cite[][ascl:9911.003]{2003ASSL..285..109G}. As discussed in \S \ref{sec:makecube}
the data are potentially spread over multiple data cubes that must be
processed independently and the resulting moment map is created by
mosaicking the individual submaps taking into account the border
regions. This is configured such that no resampling is required as
\textsc{makecube} ensures that all tiles are on the same pixel grid.

\begin{figure}
\includegraphics[width=0.46\textwidth]{integ_manual.pdf}
\includegraphics[width=0.46\textwidth]{integ_auto.pdf}
\caption{Two integrated intensity images from the same set of
  observations of Serpens from 2007. The top figure uses a naive sum
  over a significant part of the baseline. The lower figure uses the
  automated baseline masking.}
\label{fig:integ}
\end{figure}

Fig.\ \ref{fig:integ} compares integrated intensity images calculated
in two different ways from a single data cube generated by the
pipeline \citep[see][for details of earlier
reductions of these data]{2010MNRAS.409.1412G,2010A&A...523A..29D}. This data set
has some interference in a few spectral channels of a few receptors
that has not yet been handled by the main pipeline
processing. Neverthless, the lower image shows no sign of the grid
printing through and shows much more dynamic range than the naive
integrated intensity image.

\subsection{Removal of Bad-Baseline Spectra}



\textit{Malcolm's ADASS XXII poster and JCMT Newsletter article. Blog mentions
that we might also handle ringing in spectra.}

The bad baselines can be divided roughly into two classes:
high-frequency, high amplitude; and low frequency, lower
amplitude. The former appear mostly in single isolated spectra or in
narrow bands, but can also manifest as spiky spectra, or
weaker-amplitude striations persistent over tens of spectra.
Figure~\ref{fig:badbase:highfreq} presents the most-common forms.

\begin{figure}[!ht]
\includegraphics[width=0.23\textwidth]{P61_f1a}
\includegraphics[width=0.23\textwidth]{P61_f1b}
\caption{Examples of high-frequency interference in spectral-time
  axes. The left panel shows high-amplitude interference affecting
  single spectra; and part of an affected spectrum plotted below. The
  amplitude dwarfs the normal signal by an order of magnitude. The
  right panel shows bands of phase-shifting interference.}
\label{fig:badbase:highfreq}
\end{figure}

The low-frequency ripples tend to occur in time-series blocks that are
often visible because of baseline drift, but can apply to all spectra
for a receptor. They have a wide range of morphologies such as
sinusoids, irregular ripples, and curved, twin headlight-like beams
that start strong but gradually pan out and fade with time.
Figure~\ref{fig:badbase:interference} displays some examples.

\begin{figure}[!ht]
\includegraphics[width=0.23\textwidth]{P61_f2a}
\includegraphics[width=0.23\textwidth]{P61_f2b}
\caption{Examples of low-frequency interference.  The top-left graphic
  shows a band of low-frequency oscillations; below it is the band's
  average spectrum.  The lower-left spectrum has a periodic weak
  ripple hard to detect visually.  The right panel shows time-averaged
  (clipped mean) spectra for each receptor in which the fourth and
  ninth from the bottom exhibit global non-linearity.}
\label{fig:badbase:interference}
\end{figure}

\subsubsection{Adopted solution}

The pipeline applies three steps in the quality-assurance stage:
\begin{itemize}
\item Laplacian filtering of high-frequency noise
\item non-linearity detection for individual spectra
\item global non-linearity to reject whole receptors
\end{itemize}

\subsubsection{Masking of High-Frequency Noise}

The recipe applies a one-dimensional Laplacian edge filter to all the
spectra for each receptor trimming the outer 15~\% where noise is
always present.  This approximates to a difference-of-Gaussian
filter. It next sums the rms `edginess' along the spectral axis to
form a profile through the time series.  Drifts or steps in the
profile are removed.  The final step is to eject spectra whose rms
edginess exceeds the median level by a nominated number of clipped
standard deviations.  Affected spectra are easily delineated.  An
optional second iteration removes most of the striation noise once the
pronounced edginess peaks are masked.

\subsubsection{Non-linearity Filtering}

The low-frequency rippled and wobbly baselines are addressed by
determining the non-linearity of each spectrum.  First the recipe
excludes non-baseline features that would dilute the non-linearity
signal.  These comprise a threshold to remove spikes and masking a
central region where the astronomical signal may be present. It
estimates the background level, effectively smoothing to remove
structure smaller than a nominated scale.  Next it fits linear
baselines to these and calculates the rms residuals to provide a
rectified signal.  Then it averages the signal along the spectral axis
to form a non-linearity profile through the time series for each good
receptor.

The non-linear profiles are much noisier than the summed Laplacians
and discrimination is harder.  To identify anomalous spectra the
recipe reduces the noise to obtain a smooth profile, correct for
drifts or steps in the profile.  It rejects spectra whose mean
non-linearity exceeds the mean level above a nominated number of
clipped standard deviations.  The derived standard deviation allows
for positive skewness.  It applies a mask of rejected spectra to the
input cube.

The global non-linearity test is applied last so that a block of
transient highly deviant spectra will not cause the whole receptor to
be rejected.  It operates in a similar fashion to the above.  It
diverges by determining a mean rms residual from non-linearity per
detector, from which it evaluates the median and standard deviation of
the distribution of mean rms residuals from the entire observation,
and performs iterative sigma clipping above the median to reject those
detectors whose deviations from linearity are anomalous.  There is a
tunable minimum threshold.

\subsubsection{Results}

The methods appear highly effective at cleaning the pipeline
products. It has been used to re-reduce one survey and several other
datasets. Figure~\ref{fig:badbase:results} presents an example which
had originally failed quality assurance, but now can be used for
science.

\begin{figure}
\includegraphics[width=0.23\textwidth]{P61_f3a}
\includegraphics[width=0.23\textwidth]{P61_f3b}
\caption{Comparison of an integrated intensity map using the same
  percentile scaling.  The left panel shows a map without bad-baseline
  removal.  The right panel shows the same data processed using the
  new filters.}
\label{fig:badbase:results}
\end{figure}

\subsection{Flatfielding}

Early HARP data from 2006 seemed to have problems with the relative
calibration of the receptors. A self flatfielding algorithm was
developed that relied on the receptors on average seeing the same
signal for large scan maps of molecular clouds
\citep{2010MNRAS.401..455C} and this had some success in removing
striping from integrated intensity images.

Now discuss the pipeline implementation. Before and after flatfielding examples.

\subsection{Recipe Tuning}

The data reduction system can be configured by using recipe parameters
supplied to ORAC-DR in a file. Each recipe has its own set of
parameters that are supported and these can include options to bin up
the frequency scale, control the output grid, control any regridding
parameters and enable or disable flatfielding and bad baseline filtering.

\subsection{Quality Assurance Parameters}

The survey teams required a comprehensive set of quality assurance
testing to ensure that the data quality is consistent for the duration
of the surveys \citep{2008JCMTLSQA}. A number of QA tests were added
to the pipeline and a full list is shown in Table \ref{tab:qa:params}.

\begin{table}
\caption{Quality Assurance Parameters}
\label{tab:qa:params}
\begin{tabular}{ll}
BADPIX\_MAP  & \\
CALINTTOL & \textit{Not implemented?} \\
CALPEAKTOL & \textit{Not implemented?} \\
FLAGTSYSBAD & \textit{Not implemented?} \\
GOODRECEP & \textit{Not implemented?} \\
RESTOL & \textit{Not implemented?} \\
RESTOL\_SM & \textit{Not implemented?}  \\
RES\_CHAN & \textit{Not implemented?}  \\
RMSMEANTSYSTOL & \\
RMSTSYSTOL & \\
RMSTSYSTOL\_FAIL & \\
RMSTSYSTOL\_QUEST & \\
RMSVAR\_MAP & \\
RMSVAR\_RCP &  \textit{Not implemented?} \\
RMSVAR\_SPEC & \\
TSYSBAD &  \textit{Not implemented?} \\
TSYSMAX &  \textit{Not implemented?} \\
TSYSVAR &  \textit{Not implemented?} \\
VELRES
\end{tabular}
\end{table}

\subsection{Alternative Recipes}

It is not possible or even desirable for a single data reduction
technique to apply to many different types of sources and science
goals. For that reason a number of different recipes are made
available and these can be chosen by the observer in the Observing
Tool, overridden later on the command-line or by using a
configuration file at the JCMT Science Archive.

\subsubsection{Gradient}

This is the standard recipe optimized for nearby molecular clouds with
relatively narrow lines where there can be a velocity gradient across
the cloud. This velocity gradient is a major motivation for the
automated detection of baseline regions as it allows you to maximize
the baseline region rather than supplying a simple range that
encompasses all the data.

\subsubsection{Broad line}

Nearby galaxies have broad lines of many km/s so this recipe is tuned
to be less aggressive for automatic baseline subtraction than the
standard recipe that is deisgned for nearby molecular clouds. An early
form of this recipe, in the form of a standalone script, was used for
the initial Nearby Galaxy Survey data release and is documented in
\citet{2010ApJ...714..571W}.

\subsubsection{Line Forest}

Some transitions are very close together, for example methanol, and
different techniques are required for handling the automated basline
subtraction.

\textit{Do we use findback?}

\subsubsection{Continuum}

For continuum observations, such as planets, baseline subtraction
should not be done.

\subsubsection{Frequency Switch}

\textit{Does the frequency switch recipe actually work?}

\section{Processing of Historical JCMT Data}

Until ACSIS was delivered in 2005, heterodyne data were taken with a
variety of backend systems including the Acousto-Optical Spectrometer
(AOSC) and the Dutch Autocorrelation Spectrometer (DAS)
\citep{1986SPIE..598..134B}. These backends wrote data in the
Global Section Data (GSD) format \citep[e.g.][]{GSD1999} which was
understood by the SPECX data reduction package. To ensure that as much
of this historical data as possible is made available to the community
in a useable format we have developed an extension to the SMURF
package called \textsc{gsd2acsis} that converts the legacy data to the
newer ACSIS data format (which is based on the Starlink NDF format
\citep{NDF}). This enables the legacy data to be re-processed using
the modern data reduction pipeline and automatically leads to these
products being easily available from the JCMT Science Archive.

The main difficulty in supporting legacy data in the pipeline related
to the DAS splitting the bandwidth into many overlapping
subbands. ACSIS data only ever had included two subbands for hybrid
mode observations and so the subband merging algorithm had to be
extended to support arbitrary numbers of subbands.

\textit{Would be nice if we could reduce some GSD data and compare it with a
published map from SPECX. Something like Horsehead but I can't find
that actually published anywhere.}

\section{Conclusion}

With many thousands of spectra from a single observation it is
impractical to examine every spectrum manually. The data reduction
scheme described here is used continually at the JCMT Science Archive
\citep{2011ASPC..442..203E} for daily and project processing and the
tuned recipes are now generating the primary products from the
heterodyne part of the Gould's Belt JCMT Legacy Survey
\citep{2007PASP..119..855W}.

The iterative pipeline processing described in this paper demonstrates
the possibilities for advanced heterodyne cube reconstruction if we
begin to use techniques more akin to those used by iterative
map-makers for bolometer cameras
\citep[e.g.][]{2013MNRAS.430.2545C}. The next step is to explicitly
embrace such techniques, building up explicit models of the
astronomical emission and baselines and enhance this to have models
involving knowledge of which detectors have related local oscillators
and which detectors come from the same backend hardware. This latter
facility may be important as more and more detectors are added to
focal plane arrays and would be similar to dealing with readout issues
of bolometer arrays.

\textit{At one point Per was mentioning that Kriging would be a useful
  option in makecube for doing an optimal unbiased linear
  interpolation method. I wonder if we should mention it here as an
  option for future work. In terms of ``Bolometer map-maker'' we do
  have the problem that our sampling is much more course than these
  big bolometer cameras so I'm not sure whether we need to point that
  out.}

\section{Acknowledgments}

The James Clerk Maxwell Telescope is operated by the Joint Astronomy
Centre on behalf of the Science and Technology Facilities Council of
the United Kingdom, the National Research Council of Canada, and
(until 31 March 2013) the Netherlands Organisation for Scientific
Research. We thank the many JCMT support scientists and survey
scientists who have tested the pipeline. In particular we thank
Jessica Dempsey, Holly Thomas, Jan Wouterloot, Jane Buckle and
Jennifer Hatchell. We would also like to thank Jennifer Balfour and
Vincent Tilanus for their work on \textsc{gsd2acsis}.

This work was built on the Starlink Software Collection, which was
developed by the Starlink Project until 2005
\citep{1982QJRAS..23..485D,2005ASPC..347...22D,2008ASPC..394..650C}
and then opened up to the community. The source code for the Starlink
software (ascl:1110.012) and ORAC-DR is open-source and is
available on github at
\htmladdnormallink{https://github.com/Starlink}.

%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

%% \section{}
%% \label{}

%% References
%%
%% Following citation commands can be used in the body text:
%%
%%  \citet{key}  ==>>  Jones et al. (1990)
%%  \citep{key}  ==>>  (Jones et al., 1990)
%%
%% Multiple citations as normal:
%% \citep{key1,key2}         ==>> (Jones et al., 1990; Smith, 1989)
%%                            or  (Jones et al., 1990, 1991)
%%                            or  (Jones et al., 1990a,b)
%% \cite{key} is the equivalent of \citet{key} in author-year mode
%%
%% Full author lists may be forced with \citet* or \citep*, e.g.
%%   \citep*{key}            ==>> (Jones, Baker, and Williams, 1990)
%%
%% Optional notes as:
%%   \citep[chap. 2]{key}    ==>> (Jones et al., 1990, chap. 2)
%%   \citep[e.g.,][]{key}    ==>> (e.g., Jones et al., 1990)
%%   \citep[see][pg. 34]{key}==>> (see Jones et al., 1990, pg. 34)
%%  (Note: in standard LaTeX, only one note is allowed, after the ref.
%%   Here, one note is like the standard, two make pre- and post-notes.)
%%
%%   \citealt{key}          ==>> Jones et al. 1990
%%   \citealt*{key}         ==>> Jones, Baker, and Williams 1990
%%   \citealp{key}          ==>> Jones et al., 1990
%%   \citealp*{key}         ==>> Jones, Baker, and Williams, 1990
%%
%% Additional citation possibilities
%%   \citeauthor{key}       ==>> Jones et al.
%%   \citeauthor*{key}      ==>> Jones, Baker, and Williams
%%   \citeyear{key}         ==>> 1990
%%   \citeyearpar{key}      ==>> (1990)
%%   \citetext{priv. comm.} ==>> (priv. comm.)
%%   \citenum{key}          ==>> 11 [non-superscripted]
%% Note: full author lists depends on whether the bib style supports them;
%%       if not, the abbreviated list is printed even when full requested.
%%
%% For names like della Robbia at the start of a sentence, use
%%   \Citet{dRob98}         ==>> Della Robbia (1998)
%%   \Citep{dRob98}         ==>> (Della Robbia, 1998)
%%   \Citeauthor{dRob98}    ==>> Della Robbia


%% References with bibTeX database:

\bibliographystyle{model2-names}
\bibliography{acsisdr}

%% Authors are advised to submit their bibtex database files. They are
%% requested to list a bibtex style file in the manuscript if they do
%% not want to use model2-names.bst.

%% References without bibTeX database:

% \begin{thebibliography}{00}

%% \bibitem must have one of the following forms:
%%   \bibitem[Jones et al.(1990)]{key}...
%%   \bibitem[Jones et al.(1990)Jones, Baker, and Williams]{key}...
%%   \bibitem[Jones et al., 1990]{key}...
%%   \bibitem[\protect\citeauthoryear{Jones, Baker, and Williams}{Jones
%%       et al.}{1990}]{key}...
%%   \bibitem[\protect\citeauthoryear{Jones et al.}{1990}]{key}...
%%   \bibitem[\protect\astroncite{Jones et al.}{1990}]{key}...
%%   \bibitem[\protect\citename{Jones et al., }1990]{key}...
%%   \harvarditem[Jones et al.]{Jones, Baker, and Williams}{1990}{key}...
%%

% \bibitem[ ()]{}

% \end{thebibliography}

\end{document}

%%
%% End of file `elsarticle-template-2-harv.tex'.
